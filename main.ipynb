{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5777edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Setup directories - use current directory instead of sandbox\n",
    "base_dir = os.getcwd()  # Current directory where the script is located\n",
    "session_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "out_dir = os.path.join(base_dir, \"output\", session_dir)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {base_dir}\")\n",
    "print(f\"Output directory: {out_dir}\")\n",
    "\n",
    "# 2. Define category patterns and dtype maps\n",
    "categories = {\n",
    "    \"biometric\": \"api_data_aadhar_biometric_*.csv\",\n",
    "    \"enrolment\": \"api_data_aadhar_enrolment_*.csv\",\n",
    "    \"demographic\": \"api_data_aadhar_demographic_*.csv\",\n",
    "}\n",
    "\n",
    "dtype_maps = {\n",
    "    \"biometric\": {\n",
    "        \"date\": str, \"state\": str, \"district\": str, \"pincode\": str,\n",
    "        \"bio_age_5_17\": \"Int64\", \"bio_age_17_\": \"Int64\"\n",
    "    },\n",
    "    \"enrolment\": {\n",
    "        \"date\": str, \"state\": str, \"district\": str, \"pincode\": str,\n",
    "        \"age_0_5\": \"Int64\", \"age_5_17\": \"Int64\", \"age_18_greater\": \"Int64\"\n",
    "    },\n",
    "    \"demographic\": {\n",
    "        \"date\": str, \"state\": str, \"district\": str, \"pincode\": str,\n",
    "        \"demo_age_5_17\": \"Int64\", \"demo_age_17_\": \"Int64\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Build manifest and load/concatenate\n",
    "manifest_records = []\n",
    "master_dfs = {}\n",
    "\n",
    "print(\"\\nProcessing datasets...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cat, pattern in categories.items():\n",
    "    print(f\"\\nCategory: {cat.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # discover files\n",
    "    file_paths = sorted(glob.glob(os.path.join(base_dir, pattern)))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"  ‚ö† No files found for pattern: {pattern}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Found {len(file_paths)} file(s):\")\n",
    "    \n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for fp in file_paths:\n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(fp)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        # count rows (minus header)\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            row_count = sum(1 for _ in f) - 1\n",
    "        \n",
    "        print(f\"    - {os.path.basename(fp)}: {row_count:,} rows, {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        manifest_records.append({\n",
    "            \"category\": cat,\n",
    "            \"filename\": os.path.basename(fp),\n",
    "            \"file_path\": fp,\n",
    "            \"row_count\": row_count,\n",
    "            \"file_size_bytes\": file_size,\n",
    "            \"file_size_mb\": round(file_size_mb, 2)\n",
    "        })\n",
    "        total_rows += row_count\n",
    "        \n",
    "        # read chunk\n",
    "        df_chunk = pd.read_csv(fp, dtype=dtype_maps[cat])\n",
    "        dfs.append(df_chunk)\n",
    "    \n",
    "    # concatenate master DataFrame\n",
    "    if dfs:\n",
    "        df_master = pd.concat(dfs, ignore_index=True)\n",
    "        actual_rows = len(df_master)\n",
    "        \n",
    "        # verify counts\n",
    "        assert actual_rows == total_rows, (\n",
    "            f\"Row count mismatch for {cat}: expected {total_rows}, got {actual_rows}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  ‚úì Total rows for {cat}: {actual_rows:,}\")\n",
    "        print(f\"  ‚úì Columns: {', '.join(df_master.columns.tolist())}\")\n",
    "        \n",
    "        # save master\n",
    "        master_filename = f\"{cat}_master.csv\"\n",
    "        master_path = os.path.join(out_dir, master_filename)\n",
    "        df_master.to_csv(master_path, index=False)\n",
    "        print(f\"  ‚úì Saved master file: {master_filename}\")\n",
    "        \n",
    "        master_dfs[cat] = df_master\n",
    "\n",
    "# 4. Save manifest\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Creating manifest...\")\n",
    "\n",
    "if manifest_records:\n",
    "    manifest_df = pd.DataFrame(manifest_records)\n",
    "    manifest_path = os.path.join(out_dir, \"manifest.csv\")\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n‚úì Manifest created with {len(manifest_records)} file(s)\")\n",
    "    print(f\"‚úì Total categories processed: {len(master_dfs)}\")\n",
    "    print(f\"\\nSummary by category:\")\n",
    "    summary = manifest_df.groupby('category').agg({\n",
    "        'row_count': 'sum',\n",
    "        'file_size_mb': 'sum',\n",
    "        'filename': 'count'\n",
    "    }).rename(columns={'filename': 'file_count'})\n",
    "    print(summary)\n",
    "    \n",
    "    print(f\"\\n‚úì All files saved to: {out_dir}\")\n",
    "    print(f\"  - Manifest: manifest.csv\")\n",
    "    for cat in master_dfs.keys():\n",
    "        print(f\"  - Master file: {cat}_master.csv\")\n",
    "else:\n",
    "    print(\"‚ö† No datasets found to process!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df99a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "# Comprehensive geographic mapping for known variants\n",
    "geo_mapping = {\n",
    "    # State mappings\n",
    "    \"Dadra & Nagar Haveli\": \"Dadra and Nagar Haveli\",\n",
    "    \"Dadra & Nager Haveli\": \"Dadra and Nagar Haveli\",\n",
    "    \"NCT Of Delhi\": \"Delhi\",\n",
    "    \"Nct Of Delhi\": \"Delhi\",\n",
    "    \"National Capital Territory Of Delhi\": \"Delhi\",\n",
    "    \"Andaman & Nicobar Islands\": \"Andaman and Nicobar Islands\",\n",
    "    \"Jammu & Kashmir\": \"Jammu and Kashmir\",\n",
    "    \"Daman & Diu\": \"Daman and Diu\",\n",
    "    \"Chhattisgarh\": \"Chhattisgarh\",\n",
    "    \"Chattisgarh\": \"Chhattisgarh\",\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def parse_dates(df, date_col=\"date\"):\n",
    "    \"\"\"\n",
    "    Parse date column from dd-mm-yyyy format and validate.\n",
    "    Returns DataFrame with parsed dates and prints validation info.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üìÖ Parsing dates...\")\n",
    "    \n",
    "    # Parse dates\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format=\"%d-%m-%Y\", errors=\"coerce\")\n",
    "    \n",
    "    # Track invalid dates\n",
    "    invalid_dates = df[date_col].isna().sum()\n",
    "    if invalid_dates > 0:\n",
    "        print(f\"    ‚ö† Found {invalid_dates:,} invalid date(s)\")\n",
    "    \n",
    "    # Validate date range\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    date_span_days = (max_date - min_date).days\n",
    "    date_span_years = date_span_days / 365.25\n",
    "    \n",
    "    print(f\"    ‚úì Date range: {min_date.date()} to {max_date.date()}\")\n",
    "    print(f\"    ‚úì Span: {date_span_days} days ({date_span_years:.1f} years)\")\n",
    "    \n",
    "    # Count unique months and years\n",
    "    unique_months = df[date_col].dt.to_period('M').nunique()\n",
    "    unique_years = df[date_col].dt.year.nunique()\n",
    "    print(f\"    ‚úì Unique months: {unique_months}, Unique years: {unique_years}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def standardize_geography(df, geo_cols=[\"state\", \"district\"], mapping=geo_mapping):\n",
    "    \"\"\"\n",
    "    Standardize geographic columns: strip whitespace, title case, apply mappings.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üó∫Ô∏è  Standardizing geography...\")\n",
    "    \n",
    "    for col in geo_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        before_unique = df[col].nunique()\n",
    "        \n",
    "        # Strip and title case\n",
    "        df[col] = df[col].astype(str).str.strip().str.title()\n",
    "        \n",
    "        # Apply mappings\n",
    "        df[col] = df[col].replace(mapping)\n",
    "        \n",
    "        after_unique = df[col].nunique()\n",
    "        \n",
    "        print(f\"    ‚úì {col.title()}: {before_unique} ‚Üí {after_unique} unique values\")\n",
    "        \n",
    "        # Show top 5 values\n",
    "        top_values = df[col].value_counts().head(5)\n",
    "        print(f\"      Top values: {', '.join(top_values.index.tolist()[:3])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_pincode(df, pincode_col=\"pincode\"):\n",
    "    \"\"\"\n",
    "    Validate and standardize pincodes:\n",
    "    - Convert to zero-padded 6-digit strings\n",
    "    - Flag/drop invalid formats\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üìç Validating pincodes...\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Convert to string and strip\n",
    "    df[pincode_col] = df[pincode_col].astype(str).str.strip()\n",
    "    \n",
    "    # Pad with zeros to 6 digits\n",
    "    df[pincode_col] = df[pincode_col].str.zfill(6)\n",
    "    \n",
    "    # Validate format (exactly 6 digits)\n",
    "    valid_mask = df[pincode_col].str.match(r\"^\\d{6}$\", na=False)\n",
    "    invalid_count = (~valid_mask).sum()\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"    ‚ö† Found {invalid_count:,} invalid pincode(s) ({invalid_count/initial_count*100:.2f}%)\")\n",
    "        \n",
    "        # Show sample invalid pincodes\n",
    "        invalid_samples = df.loc[~valid_mask, pincode_col].unique()[:5]\n",
    "        print(f\"      Samples: {', '.join(map(str, invalid_samples))}\")\n",
    "        \n",
    "        # Drop invalid rows\n",
    "        df = df[valid_mask].copy()\n",
    "        print(f\"    ‚úì Dropped invalid rows, remaining: {len(df):,}\")\n",
    "    else:\n",
    "        print(f\"    ‚úì All pincodes valid\")\n",
    "    \n",
    "    # Validate pincode ranges (Indian pincodes: 110001-855120)\n",
    "    df_numeric = pd.to_numeric(df[pincode_col], errors='coerce')\n",
    "    min_pin = df_numeric.min()\n",
    "    max_pin = df_numeric.max()\n",
    "    print(f\"    ‚úì Pincode range: {int(min_pin):06d} to {int(max_pin):06d}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cast_count_columns(df, exclude_cols={\"date\", \"state\", \"district\", \"pincode\"}):\n",
    "    \"\"\"\n",
    "    Cast all count columns to nullable integers (Int64).\n",
    "    Coerce non-numeric values to NaN.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üî¢ Casting count columns...\")\n",
    "    \n",
    "    count_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    \n",
    "    for col in count_cols:\n",
    "        before_nulls = df[col].isna().sum()\n",
    "        \n",
    "        # Convert to numeric (coerce errors to NaN)\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        \n",
    "        after_nulls = df[col].isna().sum()\n",
    "        new_nulls = after_nulls - before_nulls\n",
    "        \n",
    "        if new_nulls > 0:\n",
    "            print(f\"    ‚ö† {col}: {new_nulls} non-numeric value(s) coerced to NaN\")\n",
    "        \n",
    "        # Cast to nullable integer\n",
    "        df[col] = df[col].astype(\"Int64\")\n",
    "    \n",
    "    print(f\"    ‚úì Converted {len(count_cols)} column(s) to Int64\")\n",
    "    print(f\"      Columns: {', '.join(count_cols)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_data_quality_report(df, category, output_path):\n",
    "    \"\"\"\n",
    "    Generate a data quality report for the cleaned dataset.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(f\"Data Quality Report: {category.upper()}\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"\\nüìä Dataset Overview\")\n",
    "    report.append(f\"  Rows: {len(df):,}\")\n",
    "    report.append(f\"  Columns: {len(df.columns)}\")\n",
    "    report.append(f\"\\nüìã Column Data Types\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        null_count = df[col].isna().sum()\n",
    "        null_pct = (null_count / len(df)) * 100\n",
    "        report.append(f\"  {col}: {dtype} (nulls: {null_count:,}, {null_pct:.2f}%)\")\n",
    "    \n",
    "    report.append(f\"\\nüìÖ Date Statistics\")\n",
    "    if 'date' in df.columns:\n",
    "        report.append(f\"  Min: {df['date'].min()}\")\n",
    "        report.append(f\"  Max: {df['date'].max()}\")\n",
    "        report.append(f\"  Unique dates: {df['date'].nunique():,}\")\n",
    "    \n",
    "    report.append(f\"\\nüó∫Ô∏è  Geographic Coverage\")\n",
    "    if 'state' in df.columns:\n",
    "        report.append(f\"  Unique states: {df['state'].nunique()}\")\n",
    "        report.append(f\"  Top 5 states:\")\n",
    "        for state, count in df['state'].value_counts().head(5).items():\n",
    "            report.append(f\"    - {state}: {count:,} records\")\n",
    "    \n",
    "    if 'district' in df.columns:\n",
    "        report.append(f\"  Unique districts: {df['district'].nunique()}\")\n",
    "    \n",
    "    if 'pincode' in df.columns:\n",
    "        report.append(f\"  Unique pincodes: {df['pincode'].nunique()}\")\n",
    "    \n",
    "    # Count columns statistics\n",
    "    count_cols = [c for c in df.columns if c not in {'date', 'state', 'district', 'pincode'}]\n",
    "    if count_cols:\n",
    "        report.append(f\"\\nüìà Count Columns Summary\")\n",
    "        for col in count_cols:\n",
    "            total = df[col].sum()\n",
    "            mean = df[col].mean()\n",
    "            report.append(f\"  {col}: Total={total:,}, Mean={mean:.1f}\")\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# ============================\n",
    "# MAIN PROCESSING\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    # Get the most recent output directory\n",
    "    base_dir = os.getcwd()\n",
    "    output_base = os.path.join(base_dir, \"output\")\n",
    "    \n",
    "    # Find most recent session directory\n",
    "    session_dirs = [d for d in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, d))]\n",
    "    if not session_dirs:\n",
    "        print(\"‚ùå No output directories found. Please run process_datasets.py first.\")\n",
    "        return\n",
    "    \n",
    "    latest_session = sorted(session_dirs)[-1]\n",
    "    out_dir = os.path.join(output_base, latest_session)\n",
    "    \n",
    "    print(f\"üìÇ Processing files from: {out_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create cleaned subdirectory\n",
    "    cleaned_dir = os.path.join(out_dir, \"cleaned\")\n",
    "    os.makedirs(cleaned_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each category\n",
    "    categories = [\"biometric\", \"enrolment\", \"demographic\"]\n",
    "    cleaned_files = []\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üîÑ Processing: {category.upper()}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Find master file\n",
    "        master_file = os.path.join(out_dir, f\"{category}_master.csv\")\n",
    "        \n",
    "        if not os.path.exists(master_file):\n",
    "            print(f\"  ‚ö† Master file not found: {master_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load data (read all as strings initially)\n",
    "        print(f\"  üì• Loading {category} data...\")\n",
    "        df = pd.read_csv(master_file, dtype=str, low_memory=False)\n",
    "        print(f\"    Initial rows: {len(df):,}\")\n",
    "        \n",
    "        # Apply cleaning steps\n",
    "        df = parse_dates(df)\n",
    "        df = standardize_geography(df)\n",
    "        df = validate_pincode(df)\n",
    "        df = cast_count_columns(df)\n",
    "        \n",
    "        # Save cleaned file\n",
    "        output_filename = f\"{category}_cleaned.csv\"\n",
    "        output_path = os.path.join(cleaned_dir, output_filename)\n",
    "        \n",
    "        print(f\"\\n  üíæ Saving cleaned data...\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"    ‚úì Saved: {output_filename}\")\n",
    "        print(f\"    ‚úì Final rows: {len(df):,}\")\n",
    "        \n",
    "        cleaned_files.append(output_filename)\n",
    "        \n",
    "        # Generate data quality report\n",
    "        report_filename = f\"{category}_quality_report.txt\"\n",
    "        report_path = os.path.join(cleaned_dir, report_filename)\n",
    "        report = generate_data_quality_report(df, category, report_path)\n",
    "        print(f\"\\n  üìã Quality report saved: {report_filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ CLEANING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìÅ Cleaned files saved to: {cleaned_dir}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    for filename in cleaned_files:\n",
    "        file_path = os.path.join(cleaned_dir, filename)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  ‚úì {filename} ({file_size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\nüìä Quality reports:\")\n",
    "    for category in categories:\n",
    "        report_file = f\"{category}_quality_report.txt\"\n",
    "        if os.path.exists(os.path.join(cleaned_dir, report_file)):\n",
    "            print(f\"  ‚úì {report_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "# Maximum gap size (in days) for which imputation is warranted\n",
    "MAX_GAP_DAYS = 30  # Only impute if gap is <= 30 days\n",
    "\n",
    "# Minimum observations required for a pincode to perform imputation\n",
    "MIN_OBSERVATIONS = 3\n",
    "\n",
    "# ============================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def analyze_missing_patterns(df, count_cols, category):\n",
    "    \"\"\"\n",
    "    Analyze missing data patterns before imputation.\n",
    "    Returns statistics about missing values.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üîç Analyzing missing patterns for {category}...\")\n",
    "    \n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'total_rows': len(df),\n",
    "        'columns_analyzed': []\n",
    "    }\n",
    "    \n",
    "    for col in count_cols:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_pct = (zero_count / len(df)) * 100\n",
    "        \n",
    "        stats['columns_analyzed'].append({\n",
    "            'column': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct,\n",
    "            'zero_count': zero_count,\n",
    "            'zero_pct': zero_pct\n",
    "        })\n",
    "        \n",
    "        print(f\"    {col}:\")\n",
    "        print(f\"      Missing (NaN): {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "        print(f\"      True Zeros: {zero_count:,} ({zero_pct:.2f}%)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def impute_pincode_level(group, count_cols, max_gap_days=MAX_GAP_DAYS):\n",
    "    \"\"\"\n",
    "    Impute missing values at the pincode level using forward/backward fill.\n",
    "    Only imputes if:\n",
    "    1. The gap is within max_gap_days\n",
    "    2. There are sufficient observations for that pincode\n",
    "    \n",
    "    Returns the group with imputed values and a tracking flag.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure proper forward/backward fill\n",
    "    group = group.sort_values('date').copy()\n",
    "    \n",
    "    # Skip if too few observations\n",
    "    if len(group) < MIN_OBSERVATIONS:\n",
    "        return group\n",
    "    \n",
    "    # Add tracking columns for each count column\n",
    "    for col in count_cols:\n",
    "        tracking_col = f\"{col}_imputed\"\n",
    "        group[tracking_col] = False\n",
    "        \n",
    "        # Identify missing values\n",
    "        missing_mask = group[col].isna()\n",
    "        \n",
    "        if missing_mask.any():\n",
    "            # Calculate time gaps between consecutive observations\n",
    "            group['date_diff'] = group['date'].diff()\n",
    "            \n",
    "            # Forward fill with condition\n",
    "            ffill_values = group[col].ffill()\n",
    "            \n",
    "            # Backward fill with condition\n",
    "            bfill_values = group[col].bfill()\n",
    "            \n",
    "            # For each missing value, check if gap is small enough\n",
    "            for idx in group[missing_mask].index:\n",
    "                # Get position in group\n",
    "                pos = group.index.get_loc(idx)\n",
    "                \n",
    "                # Check forward gap\n",
    "                if pos > 0:\n",
    "                    prev_idx = group.index[pos - 1]\n",
    "                    date_gap = (group.loc[idx, 'date'] - group.loc[prev_idx, 'date']).days\n",
    "                    \n",
    "                    if date_gap <= max_gap_days and pd.notna(ffill_values.loc[idx]):\n",
    "                        group.loc[idx, col] = ffill_values.loc[idx]\n",
    "                        group.loc[idx, tracking_col] = True\n",
    "                        continue\n",
    "                \n",
    "                # Check backward gap\n",
    "                if pos < len(group) - 1:\n",
    "                    next_idx = group.index[pos + 1]\n",
    "                    date_gap = (group.loc[next_idx, 'date'] - group.loc[idx, 'date']).days\n",
    "                    \n",
    "                    if date_gap <= max_gap_days and pd.notna(bfill_values.loc[idx]):\n",
    "                        group.loc[idx, col] = bfill_values.loc[idx]\n",
    "                        group.loc[idx, tracking_col] = True\n",
    "            \n",
    "            # Clean up temporary column\n",
    "            group = group.drop('date_diff', axis=1, errors='ignore')\n",
    "    \n",
    "    return group\n",
    "\n",
    "def create_is_missing_flag(df, count_cols):\n",
    "    \"\"\"\n",
    "    Create a consolidated 'is_missing' flag indicating if ANY imputation was performed on the row.\n",
    "    \"\"\"\n",
    "    tracking_cols = [f\"{col}_imputed\" for col in count_cols]\n",
    "    \n",
    "    # Create is_missing flag (True if any column was imputed)\n",
    "    df['is_missing'] = False\n",
    "    for tracking_col in tracking_cols:\n",
    "        if tracking_col in df.columns:\n",
    "            df['is_missing'] = df['is_missing'] | df[tracking_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_imputation_report(df, count_cols, category, pre_impute_stats, output_path):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive imputation report showing:\n",
    "    - Pre/post imputation statistics\n",
    "    - Proportion of filled vs actual data\n",
    "    - Imputation logic documentation\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(f\"Missing Data Imputation Report: {category.upper()}\")\n",
    "    report.append(\"=\" * 90)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    report.append(f\"\\nüìã IMPUTATION CONFIGURATION\")\n",
    "    report.append(f\"  Maximum gap for imputation: {MAX_GAP_DAYS} days\")\n",
    "    report.append(f\"  Minimum observations per pincode: {MIN_OBSERVATIONS}\")\n",
    "    report.append(f\"  Imputation method: Forward/Backward fill at pincode level\")\n",
    "    \n",
    "    report.append(f\"\\nüìä DATASET OVERVIEW\")\n",
    "    report.append(f\"  Total rows: {len(df):,}\")\n",
    "    report.append(f\"  Total pincodes: {df['pincode'].nunique():,}\")\n",
    "    report.append(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # Overall imputation statistics\n",
    "    if 'is_missing' in df.columns:\n",
    "        imputed_rows = df['is_missing'].sum()\n",
    "        imputed_pct = (imputed_rows / len(df)) * 100\n",
    "        report.append(f\"\\nüîß IMPUTATION SUMMARY\")\n",
    "        report.append(f\"  Rows with any imputation: {imputed_rows:,} ({imputed_pct:.2f}%)\")\n",
    "        report.append(f\"  Rows with actual data: {len(df) - imputed_rows:,} ({100 - imputed_pct:.2f}%)\")\n",
    "    \n",
    "    # Column-level statistics\n",
    "    report.append(f\"\\nüìà COLUMN-LEVEL ANALYSIS\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    for col in count_cols:\n",
    "        tracking_col = f\"{col}_imputed\"\n",
    "        \n",
    "        # Pre-imputation stats\n",
    "        pre_stats = next((s for s in pre_impute_stats['columns_analyzed'] if s['column'] == col), None)\n",
    "        \n",
    "        # Post-imputation stats\n",
    "        if tracking_col in df.columns:\n",
    "            imputed_count = df[tracking_col].sum()\n",
    "            imputed_pct = (imputed_count / len(df)) * 100\n",
    "            \n",
    "            still_missing = df[col].isna().sum()\n",
    "            still_missing_pct = (still_missing / len(df)) * 100\n",
    "        else:\n",
    "            imputed_count = 0\n",
    "            imputed_pct = 0.0\n",
    "            still_missing = df[col].isna().sum()\n",
    "            still_missing_pct = (still_missing / len(df)) * 100\n",
    "        \n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_pct = (zero_count / len(df)) * 100\n",
    "        \n",
    "        report.append(f\"  {col}:\")\n",
    "        if pre_stats:\n",
    "            report.append(f\"    Before imputation:\")\n",
    "            report.append(f\"      Missing (NaN): {pre_stats['missing_count']:,} ({pre_stats['missing_pct']:.2f}%)\")\n",
    "            report.append(f\"      True Zeros: {pre_stats['zero_count']:,} ({pre_stats['zero_pct']:.2f}%)\")\n",
    "        \n",
    "        report.append(f\"    After imputation:\")\n",
    "        report.append(f\"      Values imputed: {imputed_count:,} ({imputed_pct:.2f}%)\")\n",
    "        report.append(f\"      Still missing: {still_missing:,} ({still_missing_pct:.2f}%)\")\n",
    "        report.append(f\"      True Zeros: {zero_count:,} ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        if pre_stats:\n",
    "            filled_pct = (imputed_count / pre_stats['missing_count'] * 100) if pre_stats['missing_count'] > 0 else 0\n",
    "            report.append(f\"      Fill rate: {filled_pct:.2f}% of original missing values\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Imputation by pincode\n",
    "    report.append(f\"üìç IMPUTATION BY PINCODE\")\n",
    "    if 'is_missing' in df.columns:\n",
    "        pincode_stats = df.groupby('pincode').agg({\n",
    "            'is_missing': ['sum', 'count']\n",
    "        }).reset_index()\n",
    "        pincode_stats.columns = ['pincode', 'imputed_count', 'total_count']\n",
    "        pincode_stats['imputed_pct'] = (pincode_stats['imputed_count'] / pincode_stats['total_count']) * 100\n",
    "        \n",
    "        pincodes_with_imputation = (pincode_stats['imputed_count'] > 0).sum()\n",
    "        total_pincodes = len(pincode_stats)\n",
    "        \n",
    "        report.append(f\"  Pincodes with any imputation: {pincodes_with_imputation:,} / {total_pincodes:,}\")\n",
    "        report.append(f\"\\n  Top 10 pincodes by imputation count:\")\n",
    "        \n",
    "        top_pincodes = pincode_stats.nlargest(10, 'imputed_count')\n",
    "        for _, row in top_pincodes.iterrows():\n",
    "            report.append(f\"    {row['pincode']}: {int(row['imputed_count'])} / {int(row['total_count'])} ({row['imputed_pct']:.1f}%)\")\n",
    "    \n",
    "    # Imputation logic documentation\n",
    "    report.append(f\"\\nüìù IMPUTATION LOGIC\")\n",
    "    report.append(f\"  Strategy: Pincode-level temporal imputation\")\n",
    "    report.append(f\"  Method:\")\n",
    "    report.append(f\"    1. Group data by pincode\")\n",
    "    report.append(f\"    2. Sort chronologically by date\")\n",
    "    report.append(f\"    3. For each missing value:\")\n",
    "    report.append(f\"       - Check gap to previous observation\")\n",
    "    report.append(f\"       - If gap ‚â§ {MAX_GAP_DAYS} days, forward fill\")\n",
    "    report.append(f\"       - Otherwise, check gap to next observation\")\n",
    "    report.append(f\"       - If gap ‚â§ {MAX_GAP_DAYS} days, backward fill\")\n",
    "    report.append(f\"       - Otherwise, leave as NaN\")\n",
    "    report.append(f\"    4. Skip pincodes with < {MIN_OBSERVATIONS} observations\")\n",
    "    report.append(f\"\\n  Rationale:\")\n",
    "    report.append(f\"    - Preserves true zeros (no events) vs missing reporting (NaN)\")\n",
    "    report.append(f\"    - Only fills intermittent gaps within reasonable timeframes\")\n",
    "    report.append(f\"    - Maintains temporal consistency within each pincode\")\n",
    "    report.append(f\"    - Flags all imputed values for transparency\")\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# ============================\n",
    "# MAIN PROCESSING\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    # Get the most recent output directory\n",
    "    base_dir = os.getcwd()\n",
    "    output_base = os.path.join(base_dir, \"output\")\n",
    "    \n",
    "    # Find most recent session directory\n",
    "    session_dirs = [d for d in os.listdir(output_base) if os.path.isdir(os.path.join(output_base, d))]\n",
    "    if not session_dirs:\n",
    "        print(\"‚ùå No output directories found. Please run previous scripts first.\")\n",
    "        return\n",
    "    \n",
    "    latest_session = sorted(session_dirs)[-1]\n",
    "    cleaned_dir = os.path.join(output_base, latest_session, \"cleaned\")\n",
    "    \n",
    "    if not os.path.exists(cleaned_dir):\n",
    "        print(\"‚ùå Cleaned directory not found. Please run clean_datasets.py first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Processing files from: {cleaned_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Create imputed subdirectory\n",
    "    imputed_dir = os.path.join(cleaned_dir, \"imputed\")\n",
    "    os.makedirs(imputed_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each category\n",
    "    categories = [\"biometric\", \"enrolment\", \"demographic\"]\n",
    "    \n",
    "    # Define count columns for each category\n",
    "    count_columns = {\n",
    "        \"biometric\": [\"bio_age_5_17\", \"bio_age_17_\"],\n",
    "        \"enrolment\": [\"age_0_5\", \"age_5_17\", \"age_18_greater\"],\n",
    "        \"demographic\": [\"demo_age_5_17\", \"demo_age_17_\"]\n",
    "    }\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(f\"üîÑ Processing: {category.upper()}\")\n",
    "        print('='*90)\n",
    "        \n",
    "        # Find cleaned file\n",
    "        cleaned_file = os.path.join(cleaned_dir, f\"{category}_cleaned.csv\")\n",
    "        \n",
    "        if not os.path.exists(cleaned_file):\n",
    "            print(f\"  ‚ö† Cleaned file not found: {cleaned_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"  üì• Loading {category} data...\")\n",
    "        df = pd.read_csv(cleaned_file, parse_dates=['date'])\n",
    "        print(f\"    Rows: {len(df):,}\")\n",
    "        \n",
    "        # Get count columns for this category\n",
    "        count_cols = count_columns.get(category, [])\n",
    "        count_cols = [col for col in count_cols if col in df.columns]\n",
    "        \n",
    "        if not count_cols:\n",
    "            print(f\"  ‚ö† No count columns found for {category}\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze missing patterns (before imputation)\n",
    "        pre_impute_stats = analyze_missing_patterns(df, count_cols, category)\n",
    "        \n",
    "        # Perform imputation at pincode level\n",
    "        print(f\"\\n  üîß Performing pincode-level imputation...\")\n",
    "        print(f\"    Count columns: {', '.join(count_cols)}\")\n",
    "        \n",
    "        df_imputed = df.groupby('pincode', group_keys=False).apply(\n",
    "            lambda x: impute_pincode_level(x, count_cols, MAX_GAP_DAYS)\n",
    "        )\n",
    "        \n",
    "        # Create consolidated is_missing flag\n",
    "        print(f\"\\n  üè∑Ô∏è  Creating imputation tracking flags...\")\n",
    "        df_imputed = create_is_missing_flag(df_imputed, count_cols)\n",
    "        \n",
    "        # Count imputation results\n",
    "        total_imputed = df_imputed['is_missing'].sum()\n",
    "        imputed_pct = (total_imputed / len(df_imputed)) * 100\n",
    "        print(f\"    ‚úì Rows with imputation: {total_imputed:,} ({imputed_pct:.2f}%)\")\n",
    "        \n",
    "        # Save imputed file\n",
    "        output_filename = f\"{category}_imputed.csv\"\n",
    "        output_path = os.path.join(imputed_dir, output_filename)\n",
    "        \n",
    "        print(f\"\\n  üíæ Saving imputed data...\")\n",
    "        df_imputed.to_csv(output_path, index=False)\n",
    "        print(f\"    ‚úì Saved: {output_filename}\")\n",
    "        \n",
    "        # Generate imputation report\n",
    "        report_filename = f\"{category}_imputation_report.txt\"\n",
    "        report_path = os.path.join(imputed_dir, report_filename)\n",
    "        report = generate_imputation_report(df_imputed, count_cols, category, pre_impute_stats, report_path)\n",
    "        print(f\"\\n  üìã Imputation report saved: {report_filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"‚úÖ IMPUTATION COMPLETE\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"\\nüìÅ Imputed files saved to: {imputed_dir}\")\n",
    "    \n",
    "    # List created files\n",
    "    if os.path.exists(imputed_dir):\n",
    "        files = os.listdir(imputed_dir)\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        report_files = [f for f in files if f.endswith('.txt')]\n",
    "        \n",
    "        print(f\"\\nüìä Files created:\")\n",
    "        for filename in csv_files:\n",
    "            file_path = os.path.join(imputed_dir, filename)\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  ‚úì {filename} ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\nüìã Reports created:\")\n",
    "        for filename in report_files:\n",
    "            print(f\"  ‚úì {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "\n",
    "# Maximum gap size (in days) for which imputation is warranted\n",
    "MAX_GAP_DAYS = 30  # Only impute if gap is <= 30 days\n",
    "\n",
    "# Minimum observations required for a pincode to perform imputation\n",
    "MIN_OBSERVATIONS = 3\n",
    "\n",
    "# ============================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def analyze_missing_patterns(df, count_cols, category):\n",
    "    \"\"\"\n",
    "    Analyze missing data patterns before imputation.\n",
    "    Returns statistics about missing values.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  üîç Analyzing missing patterns for {category}...\")\n",
    "    \n",
    "    stats = {\n",
    "        'category': category,\n",
    "        'total_rows': len(df),\n",
    "        'columns_analyzed': []\n",
    "    }\n",
    "    \n",
    "    for col in count_cols:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_pct = (zero_count / len(df)) * 100\n",
    "        \n",
    "        stats['columns_analyzed'].append({\n",
    "            'column': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct,\n",
    "            'zero_count': zero_count,\n",
    "            'zero_pct': zero_pct\n",
    "        })\n",
    "        \n",
    "        print(f\"    {col}:\")\n",
    "        print(f\"      Missing (NaN): {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "        print(f\"      True Zeros: {zero_count:,} ({zero_pct:.2f}%)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def impute_pincode_level(group, count_cols, max_gap_days=MAX_GAP_DAYS):\n",
    "    \"\"\"\n",
    "    Impute missing values at the pincode level using forward/backward fill.\n",
    "    Only imputes if:\n",
    "    1. The gap is within max_gap_days\n",
    "    2. There are sufficient observations for that pincode\n",
    "    \n",
    "    Returns the group with imputed values and a tracking flag.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure proper forward/backward fill\n",
    "    group = group.sort_values('date').copy()\n",
    "    \n",
    "    # Skip if too few observations\n",
    "    if len(group) < MIN_OBSERVATIONS:\n",
    "        return group\n",
    "    \n",
    "    # Add tracking columns for each count column\n",
    "    for col in count_cols:\n",
    "        tracking_col = f\"{col}_imputed\"\n",
    "        group[tracking_col] = False\n",
    "        \n",
    "        # Identify missing values\n",
    "        missing_mask = group[col].isna()\n",
    "        \n",
    "        if missing_mask.any():\n",
    "            # Calculate time gaps between consecutive observations\n",
    "            group['date_diff'] = group['date'].diff()\n",
    "            \n",
    "            # Forward fill with condition\n",
    "            ffill_values = group[col].ffill()\n",
    "            \n",
    "            # Backward fill with condition\n",
    "            bfill_values = group[col].bfill()\n",
    "            \n",
    "            # For each missing value, check if gap is small enough\n",
    "            for idx in group[missing_mask].index:\n",
    "                # Get position in group\n",
    "                pos = group.index.get_loc(idx)\n",
    "                \n",
    "                # Check forward gap\n",
    "                if pos > 0:\n",
    "                    prev_idx = group.index[pos - 1]\n",
    "                    date_gap = (group.loc[idx, 'date'] - group.loc[prev_idx, 'date']).days\n",
    "                    \n",
    "                    if date_gap <= max_gap_days and pd.notna(ffill_values.loc[idx]):\n",
    "                        group.loc[idx, col] = ffill_values.loc[idx]\n",
    "                        group.loc[idx, tracking_col] = True\n",
    "                        continue\n",
    "                \n",
    "                # Check backward gap\n",
    "                if pos < len(group) - 1:\n",
    "                    next_idx = group.index[pos + 1]\n",
    "                    date_gap = (group.loc[next_idx, 'date'] - group.loc[idx, 'date']).days\n",
    "                    \n",
    "                    if date_gap <= max_gap_days and pd.notna(bfill_values.loc[idx]):\n",
    "                        group.loc[idx, col] = bfill_values.loc[idx]\n",
    "                        group.loc[idx, tracking_col] = True\n",
    "            \n",
    "            # Clean up temporary column\n",
    "            group = group.drop('date_diff', axis=1, errors='ignore')\n",
    "    \n",
    "    return group\n",
    "\n",
    "def create_is_missing_flag(df, count_cols):\n",
    "    \"\"\"\n",
    "    Create a consolidated 'is_missing' flag indicating if ANY imputation was performed on the row.\n",
    "    \"\"\"\n",
    "    tracking_cols = [f\"{col}_imputed\" for col in count_cols]\n",
    "    \n",
    "    # Create is_missing flag (True if any column was imputed)\n",
    "    df['is_missing'] = False\n",
    "    for tracking_col in tracking_cols:\n",
    "        if tracking_col in df.columns:\n",
    "            df['is_missing'] = df['is_missing'] | df[tracking_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_imputation_report(df, count_cols, category, pre_impute_stats, output_path):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive imputation report showing:\n",
    "    - Pre/post imputation statistics\n",
    "    - Proportion of filled vs actual data\n",
    "    - Imputation logic documentation\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(f\"Missing Data Imputation Report: {category.upper()}\")\n",
    "    report.append(\"=\" * 90)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    report.append(f\"\\nüìã IMPUTATION CONFIGURATION\")\n",
    "    report.append(f\"  Maximum gap for imputation: {MAX_GAP_DAYS} days\")\n",
    "    report.append(f\"  Minimum observations per pincode: {MIN_OBSERVATIONS}\")\n",
    "    report.append(f\"  Imputation method: Forward/Backward fill at pincode level\")\n",
    "    \n",
    "    report.append(f\"\\nüìä DATASET OVERVIEW\")\n",
    "    report.append(f\"  Total rows: {len(df):,}\")\n",
    "    report.append(f\"  Total pincodes: {df['pincode'].nunique():,}\")\n",
    "    report.append(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # Overall imputation statistics\n",
    "    if 'is_missing' in df.columns:\n",
    "        imputed_rows = df['is_missing'].sum()\n",
    "        imputed_pct = (imputed_rows / len(df)) * 100\n",
    "        report.append(f\"\\nüîß IMPUTATION SUMMARY\")\n",
    "        report.append(f\"  Rows with any imputation: {imputed_rows:,} ({imputed_pct:.2f}%)\")\n",
    "        report.append(f\"  Rows with actual data: {len(df) - imputed_rows:,} ({100 - imputed_pct:.2f}%)\")\n",
    "    \n",
    "    # Column-level statistics\n",
    "    report.append(f\"\\nüìà COLUMN-LEVEL ANALYSIS\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    for col in count_cols:\n",
    "        tracking_col = f\"{col}_imputed\"\n",
    "        \n",
    "        # Pre-imputation stats\n",
    "        pre_stats = next((s for s in pre_impute_stats['columns_analyzed'] if s['column'] == col), None)\n",
    "        \n",
    "        # Post-imputation stats\n",
    "        if tracking_col in df.columns:\n",
    "            imputed_count = df[tracking_col].sum()\n",
    "            imputed_pct = (imputed_count / len(df)) * 100\n",
    "            \n",
    "            still_missing = df[col].isna().sum()\n",
    "            still_missing_pct = (still_missing / len(df)) * 100\n",
    "        else:\n",
    "            imputed_count = 0\n",
    "            imputed_pct = 0.0\n",
    "            still_missing = df[col].isna().sum()\n",
    "            still_missing_pct = (still_missing / len(df)) * 100\n",
    "        \n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_pct = (zero_count / len(df)) * 100\n",
    "        \n",
    "        report.append(f\"  {col}:\")\n",
    "        if pre_stats:\n",
    "            report.append(f\"    Before imputation:\")\n",
    "            report.append(f\"      Missing (NaN): {pre_stats['missing_count']:,} ({pre_stats['missing_pct']:.2f}%)\")\n",
    "            report.append(f\"      True Zeros: {pre_stats['zero_count']:,} ({pre_stats['zero_pct']:.2f}%)\")\n",
    "        \n",
    "        report.append(f\"    After imputation:\")\n",
    "        report.append(f\"      Values imputed: {imputed_count:,} ({imputed_pct:.2f}%)\")\n",
    "        report.append(f\"      Still missing: {still_missing:,} ({still_missing_pct:.2f}%)\")\n",
    "        report.append(f\"      True Zeros: {zero_count:,} ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        if pre_stats:\n",
    "            filled_pct = (imputed_count / pre_stats['missing_count'] * 100) if pre_stats['missing_count'] > 0 else 0\n",
    "            report.append(f\"      Fill rate: {filled_pct:.2f}% of original missing values\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Imputation by pincode\n",
    "    report.append(f\"üìç IMPUTATION BY PINCODE\")\n",
    "    if 'is_missing' in df.columns:\n",
    "        pincode_stats = df.groupby('pincode').agg({\n",
    "            'is_missing': ['sum', 'count']\n",
    "        }).reset_index()\n",
    "        pincode_stats.columns = ['pincode', 'imputed_count', 'total_count']\n",
    "        pincode_stats['imputed_pct'] = (pincode_stats['imputed_count'] / pincode_stats['total_count']) * 100\n",
    "        \n",
    "        pincodes_with_imputation = (pincode_stats['imputed_count'] > 0).sum()\n",
    "        total_pincodes = len(pincode_stats)\n",
    "        \n",
    "        report.append(f\"  Pincodes with any imputation: {pincodes_with_imputation:,} / {total_pincodes:,}\")\n",
    "        report.append(f\"\\n  Top 10 pincodes by imputation count:\")\n",
    "        \n",
    "        top_pincodes = pincode_stats.nlargest(10, 'imputed_count')\n",
    "        for _, row in top_pincodes.iterrows():\n",
    "            report.append(f\"    {row['pincode']}: {int(row['imputed_count'])} / {int(row['total_count'])} ({row['imputed_pct']:.1f}%)\")\n",
    "    \n",
    "    # Imputation logic documentation\n",
    "    report.append(f\"\\nüìù IMPUTATION LOGIC\")\n",
    "    report.append(f\"  Strategy: Pincode-level temporal imputation\")\n",
    "    report.append(f\"  Method:\")\n",
    "    report.append(f\"    1. Group data by pincode\")\n",
    "    report.append(f\"    2. Sort chronologically by date\")\n",
    "    report.append(f\"    3. For each missing value:\")\n",
    "    report.append(f\"       - Check gap to previous observation\")\n",
    "    report.append(f\"       - If gap ‚â§ {MAX_GAP_DAYS} days, forward fill\")\n",
    "    report.append(f\"       - Otherwise, check gap to next observation\")\n",
    "    report.append(f\"       - If gap ‚â§ {MAX_GAP_DAYS} days, backward fill\")\n",
    "    report.append(f\"       - Otherwise, leave as NaN\")\n",
    "    report.append(f\"    4. Skip pincodes with < {MIN_OBSERVATIONS} observations\")\n",
    "    report.append(f\"\\n  Rationale:\")\n",
    "    report.append(f\"    - Preserves true zeros (no events) vs missing reporting (NaN)\")\n",
    "    report.append(f\"    - Only fills intermittent gaps within reasonable timeframes\")\n",
    "    report.append(f\"    - Maintains temporal consistency within each pincode\")\n",
    "    report.append(f\"    - Flags all imputed values for transparency\")\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# ============================\n",
    "# MAIN PROCESSING\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    # Get the most recent output directory\n",
    "    base_dir = os.getcwd()\n",
    "    output_base = os.path.join(base_dir, \"output\")\n",
    "    \n",
    "    # Find most recent session directory (timestamped format: YYYY-MM-DD_HH-MM-SS)\n",
    "    session_dirs = [d for d in os.listdir(output_base) \n",
    "                   if os.path.isdir(os.path.join(output_base, d)) and \n",
    "                   (d[0].isdigit() or '-' in d)]  # Filter for timestamped directories\n",
    "    \n",
    "    if not session_dirs:\n",
    "        print(\"‚ùå No output directories found. Please run previous scripts first.\")\n",
    "        return\n",
    "    \n",
    "    latest_session = sorted(session_dirs)[-1]\n",
    "    cleaned_dir = os.path.join(output_base, latest_session, \"cleaned\")\n",
    "    \n",
    "    if not os.path.exists(cleaned_dir):\n",
    "        print(f\"‚ùå Cleaned directory not found at: {cleaned_dir}\")\n",
    "        print(\"   Please run clean_datasets.py first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Processing files from: {cleaned_dir}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Create imputed subdirectory\n",
    "    imputed_dir = os.path.join(cleaned_dir, \"imputed\")\n",
    "    os.makedirs(imputed_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each category\n",
    "    categories = [\"biometric\", \"enrolment\", \"demographic\"]\n",
    "    \n",
    "    # Define count columns for each category\n",
    "    count_columns = {\n",
    "        \"biometric\": [\"bio_age_5_17\", \"bio_age_17_\"],\n",
    "        \"enrolment\": [\"age_0_5\", \"age_5_17\", \"age_18_greater\"],\n",
    "        \"demographic\": [\"demo_age_5_17\", \"demo_age_17_\"]\n",
    "    }\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(f\"üîÑ Processing: {category.upper()}\")\n",
    "        print('='*90)\n",
    "        \n",
    "        # Find cleaned file\n",
    "        cleaned_file = os.path.join(cleaned_dir, f\"{category}_cleaned.csv\")\n",
    "        \n",
    "        if not os.path.exists(cleaned_file):\n",
    "            print(f\"  ‚ö† Cleaned file not found: {cleaned_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"  üì• Loading {category} data...\")\n",
    "        df = pd.read_csv(cleaned_file, parse_dates=['date'])\n",
    "        print(f\"    Rows: {len(df):,}\")\n",
    "        \n",
    "        # Get count columns for this category\n",
    "        count_cols = count_columns.get(category, [])\n",
    "        count_cols = [col for col in count_cols if col in df.columns]\n",
    "        \n",
    "        if not count_cols:\n",
    "            print(f\"  ‚ö† No count columns found for {category}\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze missing patterns (before imputation)\n",
    "        pre_impute_stats = analyze_missing_patterns(df, count_cols, category)\n",
    "        \n",
    "        # Perform imputation at pincode level\n",
    "        print(f\"\\n  üîß Performing pincode-level imputation...\")\n",
    "        print(f\"    Count columns: {', '.join(count_cols)}\")\n",
    "        \n",
    "        df_imputed = df.groupby('pincode', group_keys=False).apply(\n",
    "            lambda x: impute_pincode_level(x, count_cols, MAX_GAP_DAYS)\n",
    "        )\n",
    "        \n",
    "        # Create consolidated is_missing flag\n",
    "        print(f\"\\n  üè∑Ô∏è  Creating imputation tracking flags...\")\n",
    "        df_imputed = create_is_missing_flag(df_imputed, count_cols)\n",
    "        \n",
    "        # Count imputation results\n",
    "        total_imputed = df_imputed['is_missing'].sum()\n",
    "        imputed_pct = (total_imputed / len(df_imputed)) * 100\n",
    "        print(f\"    ‚úì Rows with imputation: {total_imputed:,} ({imputed_pct:.2f}%)\")\n",
    "        \n",
    "        # Save imputed file\n",
    "        output_filename = f\"{category}_imputed.csv\"\n",
    "        output_path = os.path.join(imputed_dir, output_filename)\n",
    "        \n",
    "        print(f\"\\n  üíæ Saving imputed data...\")\n",
    "        df_imputed.to_csv(output_path, index=False)\n",
    "        print(f\"    ‚úì Saved: {output_filename}\")\n",
    "        \n",
    "        # Generate imputation report\n",
    "        report_filename = f\"{category}_imputation_report.txt\"\n",
    "        report_path = os.path.join(imputed_dir, report_filename)\n",
    "        report = generate_imputation_report(df_imputed, count_cols, category, pre_impute_stats, report_path)\n",
    "        print(f\"\\n  üìã Imputation report saved: {report_filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"‚úÖ IMPUTATION COMPLETE\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"\\nüìÅ Imputed files saved to: {imputed_dir}\")\n",
    "    \n",
    "    # List created files\n",
    "    if os.path.exists(imputed_dir):\n",
    "        files = os.listdir(imputed_dir)\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        report_files = [f for f in files if f.endswith('.txt')]\n",
    "        \n",
    "        print(f\"\\nüìä Files created:\")\n",
    "        for filename in csv_files:\n",
    "            file_path = os.path.join(imputed_dir, filename)\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  ‚úì {filename} ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\nüìã Reports created:\")\n",
    "        for filename in report_files:\n",
    "            print(f\"  ‚úì {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
